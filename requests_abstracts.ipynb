{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment and run the following lines of code ONCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # essential entity models downloads\n",
    "# nltk.downloader.download('maxent_ne_chunker')\n",
    "# nltk.downloader.download('words')\n",
    "# nltk.downloader.download('treebank')\n",
    "# nltk.downloader.download('maxent_treebank_pos_tagger')\n",
    "# nltk.downloader.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not all articles have a DOI and PII, that's why we'll be using the Scopus ID function to get abstracts. EID works too.\n",
    "# Check Try-Except inside the article_info() function\n",
    "\n",
    "# Set your keywords\n",
    "my_input = 'salinization AND flooding'\n",
    "# Set your API Key\n",
    "key = 'c944c310dad119e7f4ad0078f29540fa'\n",
    "\n",
    "def scopus_search(my_input: str) -> list:\n",
    "    api_resource = \"https://api.elsevier.com/content/search/scopus?\"\n",
    "    search_param = f'query=title-abs-key({my_input})'  # for example\n",
    "\n",
    "    # headers\n",
    "    headers = dict()\n",
    "    headers['X-ELS-APIKey'] = key\n",
    "    headers['X-ELS-ResourceVersion'] = 'XOCS'\n",
    "    headers['Accept'] = 'application/json'\n",
    "\n",
    "    # Set the desired number of results per page\n",
    "    results_per_page = 25\n",
    "\n",
    "    # Send the first request to get the total number of results\n",
    "    first_page_request = requests.get(api_resource + search_param + f\"&count={results_per_page}&start=0\", headers=headers)\n",
    "    first_page = json.loads(first_page_request.content.decode(\"utf-8\"))\n",
    "\n",
    "    total_results = int(first_page['search-results']['opensearch:totalResults'])\n",
    "    total_pages = (total_results // results_per_page) + 1\n",
    "\n",
    "    # List to store all articles\n",
    "    articles_list = []\n",
    "\n",
    "    print(f\"Scrapping Data Pages from Scopus using {my_input}...\")\n",
    "    # Iterate over all pages\n",
    "    for page_number in tqdm(range(total_pages)):\n",
    "        # print(f\"Page Number: {page_number} out of {total_pages}\" )\n",
    "        start_index = page_number * results_per_page\n",
    "        page_request = requests.get(api_resource + search_param + f\"&count={results_per_page}&start={start_index}\", headers=headers)\n",
    "        page = json.loads(page_request.content.decode(\"utf-8\"))\n",
    "        try:\n",
    "            articles_list.extend(page['search-results']['entry'])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(f\"Number of articles: {len(articles_list)}\")\n",
    "    return articles_list\n",
    "\n",
    "def article_info(articles_list: list) -> set:\n",
    "    print(f\"\\nGetting article titles...\")\n",
    "\n",
    "    article_title = []\n",
    "    article_doi = []\n",
    "    article_eid = []\n",
    "    article_ID = []\n",
    "    article_pii = []\n",
    "    global outliers\n",
    "    outliers = {}\n",
    "    # Access individual articles\n",
    "    for article in tqdm(range(len(articles_list))):\n",
    "        try:\n",
    "            article_pii.append( articles_list[article][\"pii\"]) #dc:title\n",
    "            article_title.append( articles_list[article][\"dc:title\"]) #dc:title\n",
    "            article_doi.append( articles_list[article][\"prism:doi\"]) #dc:title\n",
    "            article_eid.append( articles_list[article][\"eid\"]) #dc:title\n",
    "            article_ID.append( articles_list[article][\"dc:identifier\"]) #dc:title\n",
    "\n",
    "        except:\n",
    "            article_pii.append(None) #dc:title\n",
    "            article_doi.append(None)\n",
    "            article_title.append(articles_list[article][\"dc:title\"])\n",
    "            article_eid.append( articles_list[article][\"eid\"]) #dc:title\n",
    "            article_ID.append( articles_list[article][\"dc:identifier\"])\n",
    "            \n",
    "    return article_title, article_doi, article_eid, article_ID, article_pii\n",
    "\n",
    "def scopus_id_abstract_retreiver(scopus_id: str) -> str:\n",
    "    api_endpoint = f\"https://api.elsevier.com/content/abstract/scopus_id/{scopus_id}\"\n",
    "\n",
    "    # Set the headers\n",
    "    headers = {\n",
    "        \"X-ELS-APIKey\": key,\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Make the request to retrieve the abstract\n",
    "    response = requests.get(api_endpoint, headers=headers)\n",
    "    data = json.loads(response.content.decode(\"utf-8\"))\n",
    "\n",
    "    # Extract the abstract from the response\n",
    "    try:\n",
    "        abstract = data[\"abstracts-retrieval-response\"][\"coredata\"][\"dc:description\"]\n",
    "    except:\n",
    "        abstract = \"NA\"\n",
    "\n",
    "    # Return the abstract\n",
    "    return abstract\n",
    "\n",
    "def location_finder(text: str) -> dict:\n",
    "    # Load the pre-trained model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Sample text\n",
    "    sample_text = text\n",
    "\n",
    "    # Process the text\n",
    "    doc = nlp(sample_text)\n",
    "\n",
    "    # Find location words and their locations\n",
    "    # locations = [(entity.text, entity.label_) for entity in doc.ents if entity.label_ == \"GPE\" or entity.label_ == \"LOC\"]\n",
    "    locations = [entity.text for entity in doc.ents if entity.label_ == \"GPE\" or entity.label_ == \"LOC\"]\n",
    "\n",
    "    # Print the location words and their locations\n",
    "    # for word in locations:\n",
    "    #     print(f\"Location: {word}\")\n",
    "\n",
    "    # Sorting locations by frequency\n",
    "    my_dict = dict(Counter(locations))\n",
    "    sorted_dict = dict(sorted(my_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "    first_five_elements = dict(list(sorted_dict.items())[:5])\n",
    "\n",
    "    return first_five_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doi_abstract_retriever(doi: str) -> str:\n",
    "    api_endpoint = f\"https://api.elsevier.com/content/abstract/doi/{doi}\"\n",
    "\n",
    "    # Set the headers\n",
    "    headers = {\n",
    "        \"X-ELS-APIKey\": key,\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Make the request to retrieve the abstract\n",
    "    response = requests.get(api_endpoint, headers=headers)\n",
    "    data = json.loads(response.content.decode(\"utf-8\"))\n",
    "\n",
    "    # Extract the abstract from the response\n",
    "    abstract = data[\"abstracts-retrieval-response\"][\"coredata\"][\"dc:description\"]\n",
    "\n",
    "    # Return the abstract\n",
    "    return abstract\n",
    "\n",
    "def eid_abstract_retreiver(eid: str) -> str:\n",
    "    api_endpoint = f\"https://api.elsevier.com/content/abstract/eid/{eid}\"\n",
    "\n",
    "    # Set the headers\n",
    "    headers = {\n",
    "        \"X-ELS-APIKey\": key,\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Make the request to retrieve the abstract\n",
    "    response = requests.get(api_endpoint, headers=headers)\n",
    "    data = json.loads(response.content.decode(\"utf-8\"))\n",
    "\n",
    "    # Extract the abstract from the response\n",
    "    abstract = data[\"abstracts-retrieval-response\"][\"coredata\"][\"dc:description\"]\n",
    "\n",
    "    # Return the abstract\n",
    "    return abstract\n",
    "\n",
    "def pii_abstract_retreiver(pii: str) -> str:\n",
    "    api_endpoint = f\"https://api.elsevier.com/content/abstract/pii/{pii}\"\n",
    "\n",
    "    # Set the headers\n",
    "    headers = {\n",
    "        \"X-ELS-APIKey\": key,\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Make the request to retrieve the abstract\n",
    "    response = requests.get(api_endpoint, headers=headers)\n",
    "    data = json.loads(response.content.decode(\"utf-8\"))\n",
    "\n",
    "    # Extract the abstract from the response\n",
    "    abstract = data[\"abstracts-retrieval-response\"][\"coredata\"][\"dc:description\"]\n",
    "\n",
    "    # Return the abstract\n",
    "    return abstract"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping Data Pages from Scopus using salinization AND flooding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:12<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 350\n",
      "\n",
      "Getting article titles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:00<00:00, 232132.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting locations from 350 abstracts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:06,  1.15s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E1041] Expected a string, Doc, or bytes as input, but got: <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vg/p5dpncwj1nl960cnf3wq8_900000gn/T/ipykernel_73147/3449518274.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nGetting locations from {len(my_set[3])} abstracts...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscopus_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocation_finder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscopus_id_abstract_retreiver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscopus_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mlist_of_lists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vg/p5dpncwj1nl960cnf3wq8_900000gn/T/ipykernel_73147/921971696.py\u001b[0m in \u001b[0;36mlocation_finder\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# Process the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;31m# Find location words and their locations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;31m#call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m         \"\"\"\n\u001b[0;32m--> 999\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m_ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE1041\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m     def _ensure_doc_with_context(\n",
      "\u001b[0;31mValueError\u001b[0m: [E1041] Expected a string, Doc, or bytes as input, but got: <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "# Perform the search and retrieve article info\n",
    "my_set = article_info(scopus_search(my_input))\n",
    "\n",
    "# Create an empty list to store the output dictionary keys\n",
    "list_of_lists = []\n",
    "\n",
    "# Loop over the IDs and find locations\n",
    "print(f'\\nGetting locations from {len(my_set[3])} abstracts...')\n",
    "for n, scopus_id in tqdm(enumerate(my_set[3])):\n",
    "    output_dict = location_finder(scopus_id_abstract_retreiver(scopus_id))\n",
    "    list_of_lists.append(list(output_dict.keys()))\n",
    "\n",
    "print(f\"\\nMaking Dataframe...\")\n",
    "        \n",
    "# Extract first and second elements\n",
    "first_elements = [inner_list[0] if len(inner_list) > 0 else None for inner_list in list_of_lists]\n",
    "second_elements = [inner_list[1] if len(inner_list) > 1 else None for inner_list in list_of_lists]\n",
    "third_elements = [inner_list[2] if len(inner_list) > 2 else None for inner_list in list_of_lists]\n",
    "fourth_elements = [inner_list[3] if len(inner_list) > 3 else None for inner_list in list_of_lists]\n",
    "\n",
    "# Make DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Paper Title\": my_set[0],\n",
    "    \"Scopus ID\" : my_set[3],\n",
    "    \"DOI\": my_set[1],\n",
    "    \"first_location\" : first_elements,\n",
    "    \"second_location\" : second_elements,\n",
    "    \"third_location\" : third_elements,\n",
    "    \"fourth_location\": fourth_elements})\n",
    "\n",
    "# Saving file\n",
    "df.to_csv(\"output.csv\")\n",
    "\n",
    "print(\"DONE!\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request Output Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_list[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way to get locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locationtagger\n",
    "\n",
    "# initializing sample text\n",
    "sample_text = \"India has very rich and vivid culture\\\n",
    "        widely spread from Kerala to Nagaland to Haryana to Maharashtra. \" \\\n",
    "        \"Delhi being capital with Mumbai financial capital.\\\n",
    "        Can be said better than some western cities such as \" \\\n",
    "        \" Munich, London etc. Pakistan and Bangladesh share its borders\"\n",
    "\n",
    "# extracting entities.\n",
    "place_entity = locationtagger.find_locations(text = full_text)\n",
    "\n",
    "# getting all countries\n",
    "print(\"The countries in text : \")\n",
    "print(place_entity.countries)\n",
    "\n",
    "# getting all states\n",
    "print(\"The states in text : \")\n",
    "print(place_entity.regions)\n",
    "\n",
    "# getting all cities\n",
    "print(\"The cities in text : \")\n",
    "print(place_entity.cities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
