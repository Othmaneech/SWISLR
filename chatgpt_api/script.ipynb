{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LAParams\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = '/Users/othmaneechchabi/Desktop/Research/Data+/Climate+/SWISLTR/data/10.3390:w13121677.pdf'\n",
    "file2 = '/Users/othmaneechchabi/Desktop/Research/Data+/Climate+/SWISLTR/data/10.3390:rs10081280.pdf'\n",
    "file3 = '/Users/othmaneechchabi/Desktop/Research/Data+/Climate+/SWISLTR/data/10.1890:14-0239.1.pdf'\n",
    "file4 = '/Users/othmaneechchabi/Desktop/Research/Data+/Climate+/SWISLTR/data/10.1672:08-77.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_file_content(path_to_pdf):\n",
    "    # Set parameters \n",
    "    out_text = StringIO()\n",
    "    text_converter = TextConverter(PDFResourceManager(caching=True), out_text, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(PDFResourceManager(caching=True), text_converter)\n",
    "\n",
    "    fp = open(path_to_pdf, 'rb')\n",
    "\n",
    "    # Set the maximum number of pages to read\n",
    "    max_pages = 7\n",
    "\n",
    "    # Use tqdm to create a progress bar\n",
    "    # with tqdm(total=max_pages, desc=\"Extracting\") as pbar:\n",
    "    for index, page in enumerate(PDFPage.get_pages(fp, pagenos=set())):\n",
    "        interpreter.process_page(page)\n",
    "        # pbar.update(1)\n",
    "\n",
    "        # Check if the maximum number of pages has been reached\n",
    "        if index + 1 >= max_pages:\n",
    "            break\n",
    "\n",
    "    text = out_text.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    text_converter.close()\n",
    "    out_text.close()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_key = \"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\"\n",
    "\n",
    "import openai\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "def truncate_text(text, max_tokens):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return text\n",
    "    else:\n",
    "        return tokenizer.decode(tokens[:max_tokens])\n",
    "    \n",
    "def ask_question(context, question):\n",
    "    # Set up the OpenAI API client\n",
    "    openai.api_key = API_key  # Replace with your API key\n",
    "    truncated_context = truncate_text(context, 4096 - len(f\"Research Paper Abstract: \\nQuestion: {question}\"))\n",
    "\n",
    "    # Create a prompt by combining the context and the question\n",
    "    prompt = f\"Research Paper Text: {truncated_context}\\nQuestion: {question}\"\n",
    "\n",
    "    # Generate a response from the ChatGPT model\n",
    "    response = openai.Completion.create(\n",
    "        engine='text-davinci-003',  # Choose the ChatGPT model variant\n",
    "        prompt=prompt,\n",
    "        max_tokens=100,  # Adjust the response length as needed\n",
    "        temperature=0.7,  # Adjust the randomness of the response\n",
    "        n=1,  # Generate a single response\n",
    "        stop=None,  # Let the model generate a complete answer\n",
    "        timeout=10,  # Set a timeout (in seconds) for the API request\n",
    "    )\n",
    "\n",
    "    # Extract the answer from the model's response\n",
    "    answer = response.choices[0].text.strip()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6810 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: {'Country':'United States, Mexico', 'City':'None', 'County':'None', 'Location':'Colorado River Delta, Indus River Delta, Nile River Delta, Tigris-Euphrates River Delta'}\n"
     ]
    }
   ],
   "source": [
    "question = \"Can you give me the location/study area of the research in the following format (put 'None' if no location can be found): {'Country':'Country', 'City':'City', 'County':'County', 'Location':'Location'}\"\n",
    "context = get_pdf_file_content(file1)\n",
    "\n",
    "print(ask_question(context, question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5547 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: {'Country':'USA', 'State':'Alaska', 'City':'None', 'County':'None', 'Location':'Yukon-Kuskokwim Delta'}\n"
     ]
    }
   ],
   "source": [
    "question = \"Can you give me the location/study area of the research in the following format (put 'None' if no location can be found): {'Country':'Country', 'State':'State', 'City':'City', 'County':'County', 'Location':'Location'}\"\n",
    "context = get_pdf_file_content(file2)\n",
    "\n",
    "print(ask_question(context, question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7725 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: {'Country':'Canada', 'State':'Northwest Territories', 'City':'Yellowknife', 'County':'None', 'Location':'Mackenzie Delta'}\n"
     ]
    }
   ],
   "source": [
    "question = \"Can you give me the location/study area of the research in the following format (put 'None' if no location can be found): {'Country':'Country', 'State':'State', 'City':'City', 'County':'County', 'Location':'Location'}\"\n",
    "context = get_pdf_file_content(file3)\n",
    "\n",
    "print(ask_question(context, question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
