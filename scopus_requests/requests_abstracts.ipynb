{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment and run the following lines of code ONCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # essential entity models downloads\n",
    "# nltk.downloader.download('maxent_ne_chunker')\n",
    "# nltk.downloader.download('words')\n",
    "# nltk.downloader.download('treebank')\n",
    "# nltk.downloader.download('maxent_treebank_pos_tagger')\n",
    "# nltk.downloader.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# !pip3 install country_list\n",
    "\n",
    "# https://api.elsevier.com/content/search/scopus?query=title-abs-key(salinization)&apiKey=33db67edfe2549a23e9da7d09078b777"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set your keywords\n",
    "my_input = 'salinization AND flooding'\n",
    "# Set your API Key\n",
    "key = '2a0215213a533fe5b5c7cafc2655348c'\n",
    "\n",
    "# Create a session for making requests\n",
    "session = requests.Session()\n",
    "session.headers['X-ELS-APIKey'] = key\n",
    "session.headers['X-ELS-ResourceVersion'] = 'XOCS'\n",
    "session.headers['Accept'] = 'application/json'\n",
    "\n",
    "def scopus_search(my_input: str) -> list:\n",
    "    api_resource = \"https://api.elsevier.com/content/search/scopus?\"\n",
    "    search_param = f'query=title-abs-key({my_input})'  # for example\n",
    "\n",
    "    # Set the desired number of results per page\n",
    "    results_per_page = 25\n",
    "\n",
    "    # Send the first request to get the total number of results\n",
    "    first_page_request = session.get(api_resource + search_param + f\"&count={results_per_page}&start=0\")\n",
    "    first_page = json.loads(first_page_request.content.decode(\"utf-8\"))\n",
    "\n",
    "    total_results = int(first_page['search-results']['opensearch:totalResults'])\n",
    "    total_pages = (total_results // results_per_page) + 1\n",
    "\n",
    "    # List to store all articles\n",
    "    articles_list = []\n",
    "\n",
    "    print(f\"Scrapping Data Pages from Scopus using {my_input}...\")\n",
    "    # Iterate over all pages\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for page_number in tqdm(range(total_pages)):\n",
    "            start_index = page_number * results_per_page\n",
    "            page_request = session.get(api_resource + search_param + f\"&count={results_per_page}&start={start_index}\")\n",
    "            page = json.loads(page_request.content.decode(\"utf-8\"))\n",
    "            try:\n",
    "                articles_list.extend(page['search-results']['entry'])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        print(f\"Number of articles: {len(articles_list)}\")\n",
    "        return articles_list\n",
    "\n",
    "def article_info(articles_list: list) -> set:\n",
    "    print(f\"\\nGetting article titles...\")\n",
    "\n",
    "    article_title = []\n",
    "    article_doi = []\n",
    "    article_eid = []\n",
    "    article_ID = []\n",
    "    article_pii = []\n",
    "    article_url = []\n",
    "    article_creator = []\n",
    "    article_pub = []\n",
    "    article_coverDate = []\n",
    "    article_number_citations = []\n",
    "    \n",
    "\n",
    "    global outliers\n",
    "    outliers = {}\n",
    "    # Access individual articles\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for article in tqdm(range(len(articles_list))):\n",
    "            try:\n",
    "                article_pii.append(articles_list[article].get(\"pii\"))\n",
    "                article_title.append(articles_list[article].get(\"dc:title\"))\n",
    "                article_doi.append(articles_list[article].get(\"prism:doi\"))\n",
    "                article_eid.append(articles_list[article].get(\"eid\"))\n",
    "                article_ID.append(articles_list[article].get(\"dc:identifier\"))\n",
    "                article_url.append(articles_list[article].get(\"prism:url\"))\n",
    "                article_creator.append(articles_list[article].get(\"dc:creator\"))\n",
    "                article_pub.append(articles_list[article].get(\"prism:publicationName\"))\n",
    "                article_coverDate.append(articles_list[article].get(\"prism:coverDate\"))\n",
    "                article_number_citations.append(articles_list[article].get(\"citedby-count\"))\n",
    "\n",
    "            except:\n",
    "                article_pii.append(None)\n",
    "                article_doi.append(None)\n",
    "                article_title.append(articles_list[article].get(\"dc:title\"))\n",
    "                article_eid.append(articles_list[article].get(\"eid\"))\n",
    "                article_ID.append(articles_list[article].get(\"dc:identifier\"))\n",
    "                article_url.append(articles_list[article].get(\"prism:url\"))\n",
    "                article_creator.append(None)\n",
    "                article_pub.append(articles_list[article].get(\"prism:publicationName\"))\n",
    "                article_coverDate.append(articles_list[article].get(\"prism:coverDate\"))\n",
    "                article_number_citations.append(articles_list[article].get(\"citedby-count\"))\n",
    "\n",
    "\n",
    "        return (\n",
    "            article_title, article_doi, article_eid, article_ID,\n",
    "            article_pii, article_url, article_creator,\n",
    "            article_pub, article_coverDate, article_number_citations\n",
    "        )\n",
    "\n",
    "affiliation = []\n",
    "area = []\n",
    "author_count = []\n",
    "\n",
    "def scopus_id_abstract_retriever(scopus_id: str) -> str:\n",
    "    api_endpoint = f\"https://api.elsevier.com/content/abstract/scopus_id/{scopus_id}\"\n",
    "\n",
    "    # Make the request to retrieve the abstract\n",
    "    response = session.get(api_endpoint)\n",
    "    data = json.loads(response.content.decode(\"utf-8\"))\n",
    "    # Extract the abstract from the response\n",
    "    # try:\n",
    "    abstract = data[\"abstracts-retrieval-response\"][\"coredata\"][\"dc:description\"]\n",
    "    try: affiliation.append(data[\"abstracts-retrieval-response\"][\"affiliation\"][\"affilname\"])\n",
    "    except:\n",
    "        try: affiliation.append(data[\"abstracts-retrieval-response\"][\"affiliation\"])\n",
    "        except: affiliation.append(None)\n",
    "    # Study Area\n",
    "    try:\n",
    "        result = data[\"abstracts-retrieval-response\"][\"subject-areas\"][\"subject-area\"]\n",
    "        subjects = [subject[\"$\"] for subject in result]\n",
    "        area.append(\" & \".join(subjects))\n",
    "    except:\n",
    "        area.append(None)\n",
    "    # Authors\n",
    "    try: author_count.append(len(data[\"abstracts-retrieval-response\"][\"authors\"]['author']))\n",
    "    except: author_count.append(None)\n",
    "\n",
    "    # except:\n",
    "    #     abstract = \"NA\"\n",
    "    #     try: affiliation.append(data[\"abstracts-retrieval-response\"][\"affiliation\"][\"affilname\"])\n",
    "    #     except:\n",
    "    #         try: affiliation.append(data[\"abstracts-retrieval-response\"][\"affiliation\"])\n",
    "    #         except: affiliation.append(None)\n",
    "    #     # Study Area\n",
    "    #     try:\n",
    "    #         result = data[\"abstracts-retrieval-response\"][\"subject-areas\"][\"subject-area\"]\n",
    "    #         subjects = [subject[\"$\"] for subject in result]\n",
    "    #         area.append(\" & \".join(subjects))\n",
    "    #     except:\n",
    "    #         area.append(None)\n",
    "    #     # Authors\n",
    "    #     try: author_count.append(len(data[\"abstracts-retrieval-response\"][\"authors\"]['author']))\n",
    "    #     except: author_count.append(None)\n",
    "        \n",
    "    # Return the abstract\n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer\n",
    "\n",
    "def extract_text_from_pages(pdf_path, num_pages):\n",
    "    text = \"\"\n",
    "    for page_layout in extract_pages(pdf_path):\n",
    "        if page_layout.pageid > num_pages:\n",
    "            break\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                text += element.get_text().strip() + \" \"\n",
    "    return text.strip()\n",
    "\n",
    "def location_finder(text: str) -> dict:\n",
    "    # Load the pre-trained model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Sample text\n",
    "    sample_text = text\n",
    "\n",
    "    # Process the text\n",
    "    doc = nlp(sample_text)\n",
    "\n",
    "    # Find location words and their locations\n",
    "    locations = [entity.text for entity in doc.ents if entity.label_ == \"GPE\" or entity.label_ == \"LOC\"]\n",
    "\n",
    "    # Sorting locations by frequency\n",
    "    my_dict = dict(Counter(locations))\n",
    "    sorted_dict = dict(sorted(my_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "    first_five_elements = dict(list(sorted_dict.items())[:5])\n",
    "\n",
    "    return first_five_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT API?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_key = \"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\"\n",
    "\n",
    "import openai\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "def truncate_text(text, max_tokens):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return text\n",
    "    else:\n",
    "        return tokenizer.decode(tokens[:max_tokens])\n",
    "\n",
    "# In this updated code, a new function truncate_text is introduced, which takes the text and max_tokens\n",
    "# as input and truncates the text to fit within the maximum token limit. It uses the GPT-2 tokenizer\n",
    "# to count the tokens and then truncates the text accordingly. If the text is already within the token limit,\n",
    "# it returns the original text.\n",
    "\n",
    "# The context extracted from the PDF is passed to truncate_text to get the truncated_context.\n",
    "# The length of the prompt is then checked to ensure it does not exceed 4096 tokens. If it does,\n",
    "# the truncated_context is used in the prompt.\n",
    "\n",
    "def ask_question(context, question):\n",
    "    # Set up the OpenAI API client\n",
    "    openai.api_key = API_key  # Replace with your API key\n",
    "    truncated_context = truncate_text(context, 4096 - len(f\"Research Paper Abstract: \\nQuestion: {question}\\nAnswer:\"))\n",
    "\n",
    "    # Create a prompt by combining the context and the question\n",
    "    prompt = f\"Research Paper Abstract: {truncated_context}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "    # Generate a response from the ChatGPT model\n",
    "    response = openai.Completion.create(\n",
    "        engine='text-davinci-003',  # Choose the ChatGPT model variant\n",
    "        prompt=prompt,\n",
    "        max_tokens=100,  # Adjust the response length as needed\n",
    "        temperature=0.7,  # Adjust the randomness of the response\n",
    "        n=1,  # Generate a single response\n",
    "        stop=None,  # Let the model generate a complete answer\n",
    "        timeout=10,  # Set a timeout (in seconds) for the API request\n",
    "    )\n",
    "\n",
    "    # Extract the answer from the model's response\n",
    "    answer = response.choices[0].text.strip()\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping Data Pages from Scopus using salinization AND flooding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:07<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 353\n",
      "\n",
      "Getting article titles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 353/353 [00:00<00:00, 145928.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting locations from 353 abstracts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "353it [04:07,  1.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# Perform the search and retrieve article info\n",
    "my_set = article_info(scopus_search(my_input))\n",
    "\n",
    "# Create an empty list to store the output dictionary keys\n",
    "list_of_lists = []\n",
    "\n",
    "question = \"I'm making a map of all research about Salt Water Intrusion and Sea Level Rise (SWISLR). Using this abstract, tell me in which coastal region this SWISLR Research is done? I want your answer to be concise in this format: {Country: Country Name, State: State Name, City: City Name}. Write (None) if you can't find this information\"\n",
    "\n",
    "# Loop over the IDs and find locations\n",
    "print(f'\\nGetting locations from {len(my_set[3])} abstracts...')\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for n, scopus_id in tqdm(enumerate(my_set[3])):\n",
    "        output_dict = ask_question(scopus_id_abstract_retriever(scopus_id), question)\n",
    "        list_of_lists.append(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dc:description'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scopus_id_abstract_retriever(\u001b[39m'\u001b[39;49m\u001b[39mSCOPUS_ID:0017628037\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[57], line 113\u001b[0m, in \u001b[0;36mscopus_id_abstract_retriever\u001b[0;34m(scopus_id)\u001b[0m\n\u001b[1;32m    110\u001b[0m data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(response\u001b[39m.\u001b[39mcontent\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    111\u001b[0m \u001b[39m# Extract the abstract from the response\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m# try:\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m abstract \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39;49m\u001b[39mabstracts-retrieval-response\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mcoredata\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mdc:description\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m    114\u001b[0m \u001b[39mtry\u001b[39;00m: affiliation\u001b[39m.\u001b[39mappend(data[\u001b[39m\"\u001b[39m\u001b[39mabstracts-retrieval-response\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39maffiliation\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39maffilname\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    115\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dc:description'"
     ]
    }
   ],
   "source": [
    "scopus_id_abstract_retriever('SCOPUS_ID:0017628037')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(f\"\\nMaking Dataframe...\")\n",
    "        \n",
    "# Extract first and second elements\n",
    "first_elements = [inner_list[0] if len(inner_list) > 0 else None for inner_list in list_of_lists]\n",
    "second_elements = [inner_list[1] if len(inner_list) > 1 else None for inner_list in list_of_lists]\n",
    "third_elements = [inner_list[2] if len(inner_list) > 2 else None for inner_list in list_of_lists]\n",
    "fourth_elements = [inner_list[3] if len(inner_list) > 3 else None for inner_list in list_of_lists]\n",
    "\n",
    "# Make DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Paper Title\": my_set[0],\n",
    "    \"Scopus ID\" : my_set[3],\n",
    "    \"DOI\": my_set[1],\n",
    "    \"URL\": my_set[5],\n",
    "    \"Lead Author\": my_set[6],\n",
    "    \"Affiliation\": affiliation,\n",
    "    \"Author count\": author_count,\n",
    "    \"Area of Study\": area,\n",
    "    \"Publication\": my_set[7],\n",
    "    \"Cover Date\": my_set[8],\n",
    "    \"Number of citations\": my_set[9],\n",
    "    \"first_location\" : first_elements,\n",
    "    \"second_location\" : second_elements,\n",
    "    \"third_location\" : third_elements,\n",
    "    \"fourth_location\": fourth_elements})\n",
    "\n",
    "# Saving file\n",
    "\n",
    "affiliation_series = df['Affiliation']\n",
    "\n",
    "modified_series = affiliation_series.apply(lambda x: x[0]['affilname'] if isinstance(x, list) and len(x) > 0 else x)\n",
    "\n",
    "df['Affiliation'] = modified_series\n",
    "\n",
    "df.to_csv(\"output.csv\")\n",
    "\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way to get locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locationtagger\n",
    "\n",
    "# initializing sample text\n",
    "sample_text = \"India has very rich and vivid culture\\\n",
    "        widely spread from Kerala to Nagaland to Haryana to Maharashtra. \" \\\n",
    "        \"Delhi being capital with Mumbai financial capital.\\\n",
    "        Can be said better than some western cities such as \" \\\n",
    "        \" Munich, London etc. Pakistan and Bangladesh share its borders\"\n",
    "\n",
    "# extracting entities.\n",
    "place_entity = locationtagger.find_locations(text = full_text)\n",
    "\n",
    "# getting all countries\n",
    "print(\"The countries in text : \")\n",
    "print(place_entity.countries)\n",
    "\n",
    "# getting all states\n",
    "print(\"The states in text : \")\n",
    "print(place_entity.regions)\n",
    "\n",
    "# getting all cities\n",
    "print(\"The cities in text : \")\n",
    "print(place_entity.cities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
