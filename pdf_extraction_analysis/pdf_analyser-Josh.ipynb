{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Dependencies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp39-cp39-macosx_10_9_x86_64.whl (360 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.3/360.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.20 in /Users/joshmanto/opt/anaconda3/lib/python3.9/site-packages (from openai) (2.28.1)\n",
      "Requirement already satisfied: tqdm in /Users/joshmanto/opt/anaconda3/lib/python3.9/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/joshmanto/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/joshmanto/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/joshmanto/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/joshmanto/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (2022.9.24)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp39-cp39-macosx_10_9_x86_64.whl (36 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/joshmanto/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (21.4.0)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.2-cp39-cp39-macosx_10_9_x86_64.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-macosx_10_9_x86_64.whl (29 kB)\n",
      "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.8 yarl-1.9.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# !pip install openai\n",
    "# !pip install chatgpt \n",
    "# !pip install pdfminer\n",
    "# !pip install tqdm\n",
    "# !pip install python-time\n",
    "# !pip install nltk\n",
    "\n",
    "#updated by Josh Jul 12, 2023. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LAParams\n",
    "from tqdm import tqdm\n",
    "from io import StringIO\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "import concurrent.futures\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "#from chatgpt import ChatGPT\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boilerplat Function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_file_content(path_to_pdf):\n",
    "    # Set parameters \n",
    "    out_text = StringIO()\n",
    "    text_converter = TextConverter(PDFResourceManager(caching=True), out_text, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(PDFResourceManager(caching=True), text_converter)\n",
    "\n",
    "    fp = open(path_to_pdf, 'rb')\n",
    "\n",
    "    # Set the maximum number of pages to read\n",
    "    max_pages = 7\n",
    "\n",
    "    # Use tqdm to create a progress bar\n",
    "    # with tqdm(total=max_pages, desc=\"Extracting\") as pbar:\n",
    "    for index, page in enumerate(PDFPage.get_pages(fp, pagenos=set())):\n",
    "        interpreter.process_page(page)\n",
    "        # pbar.update(1)\n",
    "\n",
    "        # Check if the maximum number of pages has been reached\n",
    "        if index + 1 >= max_pages:\n",
    "            break\n",
    "\n",
    "    text = out_text.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    text_converter.close()\n",
    "    out_text.close()\n",
    "\n",
    "    return text #we take text and send it to chatgpt API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatGPT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"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\"\n",
    "model_engine = \"text-davinci-003\"\n",
    "prompt = \"I will be feeding you some text extracted from a research paper. The topic of the text is about SWISLR salt water intrusion and sea level rise. Your job is to identify the location or locations where SWISLR field study was conducted. Output the location in the format of Statename, Abbrv like New York, NY if possible. \"\n",
    "\n",
    "# Generate a response\n",
    "completion = openai.Completion.create(\n",
    "    engine=model_engine,\n",
    "    prompt=prompt,\n",
    "    max_tokens=1024,\n",
    "    n=1,\n",
    "    stop=None,\n",
    "    temperature=0.5,\n",
    ")\n",
    "\n",
    "response = completion.choices[0].text\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/r8/pljzgns51vd9369x9rngss5m0000gn/T/ipykernel_36127/179404999.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#run the analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_for_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "#edit keywords here \n",
    "keywords1 = [\"salt water intrusion\", \"sea level rise\", \"NACP\"]\n",
    "\n",
    "#define searching for keywords with chatgpt\n",
    "def search_for_keywords(keywords, text):\n",
    "    client = chatgpt.ChatGPT()\n",
    "    results = client.search(keywords, text=text)\n",
    "    return results\n",
    "\n",
    "#run the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    results = search_for_keywords(keywords1, text)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_newlines(text):\n",
    "    # Replace newlines in the middle of a sentence with spaces\n",
    "    text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
    "\n",
    "    # Remove newlines at the end of a sentence\n",
    "    text = re.sub(r'\\n$', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def most_recurrent_locations(text: str) -> dict:\n",
    "    # Load the pre-trained model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Replace newlines in the text\n",
    "    sample_text = replace_newlines(text)\n",
    "\n",
    "    # Process the text\n",
    "    doc = nlp(sample_text)\n",
    "\n",
    "    # Find location words and their locations\n",
    "    locations = [entity.text for entity in doc.ents if entity.label_ == \"GPE\" or entity.label_ == \"LOC\"]\n",
    "\n",
    "    # Disregard strings that contain specific words\n",
    "    disregarded_words = ['USA', 'United States', 'United States of America', 'North America', 'UNITED STATES', 'al.', 'US', \"U.S.\", 'the United States']\n",
    "    locations = [location for location in locations if not any(word in location for word in disregarded_words)]\n",
    "\n",
    "    # Count the frequency of each location\n",
    "    location_counts = Counter(locations)\n",
    "\n",
    "    # Sorting locations by frequency\n",
    "    sorted_locations = sorted(location_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Create a dictionary with 7 item (frequency) format\n",
    "    first_elements = {f\"{item} ({count})\": count for item, count in sorted_locations[:7]}\n",
    "\n",
    "    return first_elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_pdf_file_names():\n",
    "    pdf_files = glob.glob(\"/Users/othmaneechchabi/Desktop/Research/Data+/Climate+/SWISLTR/script/combined_data/*.pdf\")\n",
    "    return pdf_files\n",
    "\n",
    "# Define a function to process a single file\n",
    "def process_pdf_file(file):\n",
    "    content = get_pdf_file_content(file)\n",
    "    output_dict = most_recurrent_locations(content)\n",
    "    return file, list(output_dict.keys())\n",
    "\n",
    "pdf_files = get_pdf_file_names()\n",
    "list_of_lists = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = []\n",
    "# Create a ThreadPoolExecutor with the maximum number of worker threads\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Submit the file processing tasks to the executor\n",
    "    future_results = [executor.submit(process_pdf_file, file) for file in pdf_files]\n",
    "\n",
    "    # Use tqdm to track the progress of the tasks\n",
    "    for future in tqdm(concurrent.futures.as_completed(future_results), total=len(future_results)):\n",
    "        # Retrieve the result from the completed task and append it to the list\n",
    "        list_of_lists.append(future.result()[1])\n",
    "        filename.append(future.result()[0])\n",
    "\n",
    "# Find the maximum number of elements in the lists\n",
    "max_list_length = max(len(lst) for lst in list_of_lists)\n",
    "\n",
    "# Add empty strings to lists lacking elements\n",
    "for lst in list_of_lists:\n",
    "    while len(lst) < max_list_length:\n",
    "        lst.append('')\n",
    "\n",
    "# Now all the lists inside list_of_lists have the same number of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = []\n",
    "# list_of_lists = []\n",
    "\n",
    "# # Process each file sequentially\n",
    "# for file in pdf_files:\n",
    "#     result = process_pdf_file(file)\n",
    "#     list_of_lists.append(result[1])\n",
    "#     filename.append(result[0])\n",
    "\n",
    "# # Find the maximum number of elements in the lists\n",
    "# max_list_length = max(len(lst) for lst in list_of_lists)\n",
    "\n",
    "# # Add empty strings to lists lacking elements\n",
    "# for lst in list_of_lists:\n",
    "#     while len(lst) < max_list_length:\n",
    "#         lst.append('')\n",
    "\n",
    "\n",
    "# pdf_files = glob.glob(\"PDF Papers (20)/*.pdf\")\n",
    "# file = pdf_files[0]\n",
    "# text_file = open(\"sample.txt\", \"w\")\n",
    "# n = text_file.write(get_pdf_file_content(file))\n",
    "# text_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'list_of_lists' now contains the results from processing each PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Making Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Remove \"PDF Papers (20)\" from the strings in pdf_files\n",
    "filename = [file.replace(\"PDF Papers (20)/\", \"\") for file in filename]\n",
    "\n",
    "# Extract the columns from list_of_lists\n",
    "col1 = [item[0] if len(item) > 0 else '' for item in list_of_lists]\n",
    "col2 = [item[1] if len(item) > 1 else '' for item in list_of_lists]\n",
    "col3 = [item[2] if len(item) > 2 else '' for item in list_of_lists]\n",
    "col4 = [item[3] if len(item) > 3 else '' for item in list_of_lists]\n",
    "col5 = [item[4] if len(item) > 4 else '' for item in list_of_lists]\n",
    "col6 = [item[5] if len(item) > 5 else '' for item in list_of_lists]\n",
    "col7 = [item[6] if len(item) > 6 else '' for item in list_of_lists]\n",
    "\n",
    "# Create the dataframe\n",
    "data = {\n",
    "    'PDF File': filename,\n",
    "    'Col1': col1,\n",
    "    'Col2': col2,\n",
    "    'Col3': col3,\n",
    "    'Col4': col4,\n",
    "    'Col5': col5,\n",
    "    'Col6': col6,\n",
    "    'Col7': col7\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('text_analysis_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
