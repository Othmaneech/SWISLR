{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Dependencies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chatgpt\n",
      "  Downloading chatgpt-2.2212.0-py3-none-any.whl (24 kB)\n",
      "Collecting rich\n",
      "  Downloading rich-13.4.2-py3-none-any.whl (239 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tls-client\n",
      "  Downloading tls_client-0.2.1-py3-none-any.whl (35.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pygments<3.0.0,>=2.13.0\n",
      "  Downloading Pygments-2.15.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: tls-client, pygments, mdurl, markdown-it-py, rich, chatgpt\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.11.2\n",
      "    Uninstalling Pygments-2.11.2:\n",
      "      Successfully uninstalled Pygments-2.11.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed chatgpt-2.2212.0 markdown-it-py-3.0.0 mdurl-0.1.2 pygments-2.15.1 rich-13.4.2 tls-client-0.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# !pip install chatgpt \n",
    "# !pip install pdfminer\n",
    "# !pip install tqdm\n",
    "# !pip install python-time\n",
    "# !pip install nltk\n",
    "\n",
    "#updated by Josh Jul 12, 2023. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LAParams\n",
    "from tqdm import tqdm\n",
    "from io import StringIO\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "import concurrent.futures\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boilerplat Function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_file_content(path_to_pdf):\n",
    "    # Set parameters \n",
    "    out_text = StringIO()\n",
    "    text_converter = TextConverter(PDFResourceManager(caching=True), out_text, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(PDFResourceManager(caching=True), text_converter)\n",
    "\n",
    "    fp = open(path_to_pdf, 'rb')\n",
    "\n",
    "    # Set the maximum number of pages to read\n",
    "    max_pages = 7\n",
    "\n",
    "    # Use tqdm to create a progress bar\n",
    "    # with tqdm(total=max_pages, desc=\"Extracting\") as pbar:\n",
    "    for index, page in enumerate(PDFPage.get_pages(fp, pagenos=set())):\n",
    "        interpreter.process_page(page)\n",
    "        # pbar.update(1)\n",
    "\n",
    "        # Check if the maximum number of pages has been reached\n",
    "        if index + 1 >= max_pages:\n",
    "            break\n",
    "\n",
    "    text = out_text.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    text_converter.close()\n",
    "    out_text.close()\n",
    "\n",
    "    return text #we take text and send it to chatgpt API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatGPT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatgpt import ChatGPT\n",
    "\n",
    "#edit keywords here \n",
    "keywords1 = [\"salt water intrusion\", \"sea level rise\", \"NACP\"]\n",
    "\n",
    "#define searching for keywords with chatgpt\n",
    "def search_for_keywords(keywords, text):\n",
    "    client = chatgpt.ChatGPT()\n",
    "    results = client.search(keywords, text=text)\n",
    "    return results\n",
    "\n",
    "#run the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    results = search_for_keywords(keywords1, text)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_newlines(text):\n",
    "    # Replace newlines in the middle of a sentence with spaces\n",
    "    text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
    "\n",
    "    # Remove newlines at the end of a sentence\n",
    "    text = re.sub(r'\\n$', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def most_recurrent_locations(text: str) -> dict:\n",
    "    # Load the pre-trained model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Replace newlines in the text\n",
    "    sample_text = replace_newlines(text)\n",
    "\n",
    "    # Process the text\n",
    "    doc = nlp(sample_text)\n",
    "\n",
    "    # Find location words and their locations\n",
    "    locations = [entity.text for entity in doc.ents if entity.label_ == \"GPE\" or entity.label_ == \"LOC\"]\n",
    "\n",
    "    # Disregard strings that contain specific words\n",
    "    disregarded_words = ['USA', 'United States', 'United States of America', 'North America', 'UNITED STATES', 'al.', 'US', \"U.S.\", 'the United States']\n",
    "    locations = [location for location in locations if not any(word in location for word in disregarded_words)]\n",
    "\n",
    "    # Count the frequency of each location\n",
    "    location_counts = Counter(locations)\n",
    "\n",
    "    # Sorting locations by frequency\n",
    "    sorted_locations = sorted(location_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Create a dictionary with 7 item (frequency) format\n",
    "    first_elements = {f\"{item} ({count})\": count for item, count in sorted_locations[:7]}\n",
    "\n",
    "    return first_elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_pdf_file_names():\n",
    "    pdf_files = glob.glob(\"/Users/othmaneechchabi/Desktop/Research/Data+/Climate+/SWISLTR/script/combined_data/*.pdf\")\n",
    "    return pdf_files\n",
    "\n",
    "# Define a function to process a single file\n",
    "def process_pdf_file(file):\n",
    "    content = get_pdf_file_content(file)\n",
    "    output_dict = most_recurrent_locations(content)\n",
    "    return file, list(output_dict.keys())\n",
    "\n",
    "pdf_files = get_pdf_file_names()\n",
    "list_of_lists = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = []\n",
    "# Create a ThreadPoolExecutor with the maximum number of worker threads\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Submit the file processing tasks to the executor\n",
    "    future_results = [executor.submit(process_pdf_file, file) for file in pdf_files]\n",
    "\n",
    "    # Use tqdm to track the progress of the tasks\n",
    "    for future in tqdm(concurrent.futures.as_completed(future_results), total=len(future_results)):\n",
    "        # Retrieve the result from the completed task and append it to the list\n",
    "        list_of_lists.append(future.result()[1])\n",
    "        filename.append(future.result()[0])\n",
    "\n",
    "# Find the maximum number of elements in the lists\n",
    "max_list_length = max(len(lst) for lst in list_of_lists)\n",
    "\n",
    "# Add empty strings to lists lacking elements\n",
    "for lst in list_of_lists:\n",
    "    while len(lst) < max_list_length:\n",
    "        lst.append('')\n",
    "\n",
    "# Now all the lists inside list_of_lists have the same number of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = []\n",
    "# list_of_lists = []\n",
    "\n",
    "# # Process each file sequentially\n",
    "# for file in pdf_files:\n",
    "#     result = process_pdf_file(file)\n",
    "#     list_of_lists.append(result[1])\n",
    "#     filename.append(result[0])\n",
    "\n",
    "# # Find the maximum number of elements in the lists\n",
    "# max_list_length = max(len(lst) for lst in list_of_lists)\n",
    "\n",
    "# # Add empty strings to lists lacking elements\n",
    "# for lst in list_of_lists:\n",
    "#     while len(lst) < max_list_length:\n",
    "#         lst.append('')\n",
    "\n",
    "\n",
    "# pdf_files = glob.glob(\"PDF Papers (20)/*.pdf\")\n",
    "# file = pdf_files[0]\n",
    "# text_file = open(\"sample.txt\", \"w\")\n",
    "# n = text_file.write(get_pdf_file_content(file))\n",
    "# text_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'list_of_lists' now contains the results from processing each PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Making Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Remove \"PDF Papers (20)\" from the strings in pdf_files\n",
    "filename = [file.replace(\"PDF Papers (20)/\", \"\") for file in filename]\n",
    "\n",
    "# Extract the columns from list_of_lists\n",
    "col1 = [item[0] if len(item) > 0 else '' for item in list_of_lists]\n",
    "col2 = [item[1] if len(item) > 1 else '' for item in list_of_lists]\n",
    "col3 = [item[2] if len(item) > 2 else '' for item in list_of_lists]\n",
    "col4 = [item[3] if len(item) > 3 else '' for item in list_of_lists]\n",
    "col5 = [item[4] if len(item) > 4 else '' for item in list_of_lists]\n",
    "col6 = [item[5] if len(item) > 5 else '' for item in list_of_lists]\n",
    "col7 = [item[6] if len(item) > 6 else '' for item in list_of_lists]\n",
    "\n",
    "# Create the dataframe\n",
    "data = {\n",
    "    'PDF File': filename,\n",
    "    'Col1': col1,\n",
    "    'Col2': col2,\n",
    "    'Col3': col3,\n",
    "    'Col4': col4,\n",
    "    'Col5': col5,\n",
    "    'Col6': col6,\n",
    "    'Col7': col7\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('text_analysis_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
